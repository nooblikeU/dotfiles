#!/usr/bin/python
import requests
from bs4 import BeautifulSoup
import os
import sys
from urllib.parse import urljoin, urlparse
from multiprocessing.pool import Pool
#from timer import timer

URL = sys.argv[1]
PATH = "/home/john/.scripts/images"

def s(URL, PATH):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'}

    r = requests.get(URL, headers=headers, stream=True)
    s = BeautifulSoup(r.content, 'html.parser')
    imgs = s.find_all("img")
    if r.status_code == 200:
        for img in imgs:
            img_url = img.attrs.get("src")
            if '_d' not in img_url:
                filename = img_url.split("/")[-1]
                if filename[-3:] == "png" or filename[-4:] == "jpeg":
                    img_url = urljoin(URL, img_url)
                   # print(img_url)
                    savefile = os.path.join(PATH, filename)
                    if not os.path.isfile(savefile):
                        r = requests.get(img_url, headers=headers, stream=True)
                        r.raw.decode_content = True
                        with open(savefile, 'wb') as f:
                            f.write(r.content)
                            print('Downloading:', filename)
                    else:
                        print(filename, 'is already exists skipping download')


    print('Download Completed!')

#@timer(1, 1)
def main():
    with Pool() as pool:
        pool.starmap(s, [(URL, PATH)])

main()
